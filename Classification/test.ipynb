{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2109cee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.3 MB 18 kB/s s eta 0:00:01     |████████████▊                   | 182.6 MB 75.2 MB/s eta 0:00:04��█████████████▎              | 247.1 MB 79.1 MB/s eta 0:00:03��█████████████▌              | 251.0 MB 79.1 MB/s eta 0:00:03     |██████████████████████████████▎ | 434.0 MB 66.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 61.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.15.2)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 61.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.40.0-cp36-cp36m-manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 61.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.14.0-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 81.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 274 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 64.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (49.6.0.post20210108)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 82.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 81.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 64.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 82.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 82.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.0)\n",
      "Building wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=e4dbea4afb43492be2191faca98759c1bae493d606f78c01c83b3d0ed17e2286\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=cd55aa9a5906c508e34468995beceec013e562cb333142f9e2fcd6ea2f907ae7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.14.0 astunparse-1.6.3 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 grpcio-1.40.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acc4ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "\n",
    "## Defining the bucket \n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-445'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the csv file \n",
    "file_key = 'Fall_2021/In_Class_Assignments/framingham.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "## Reading the csv file\n",
    "heart = pd.read_csv(file_content_stream)\n",
    "heart = heart.dropna()\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f3d73df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "X = heart[['age', 'currentSmoker', 'totChol', 'BMI', 'heartRate']]\n",
    "Y = heart['TenYearCHD']\n",
    "\n",
    "## Splitting the data \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b998702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming the input variables\n",
    "scaler_train = MinMaxScaler(feature_range = (0, 1)).fit(X_train)\n",
    "X_train = scaler_train.transform(X_train)\n",
    "\n",
    "scaler_test = MinMaxScaler(feature_range = (0, 1)).fit(X_test)\n",
    "X_test = scaler_train.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0acf3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 0.5117 - accuracy: 0.8376 - val_loss: 0.4695 - val_accuracy: 0.8374\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.8502 - val_loss: 0.4483 - val_accuracy: 0.8374\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4250 - accuracy: 0.8502 - val_loss: 0.4446 - val_accuracy: 0.8374\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4219 - accuracy: 0.8502 - val_loss: 0.4438 - val_accuracy: 0.8374\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.8502 - val_loss: 0.4435 - val_accuracy: 0.8374\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8502 - val_loss: 0.4432 - val_accuracy: 0.8374\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8502 - val_loss: 0.4428 - val_accuracy: 0.8374\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4188 - accuracy: 0.8502 - val_loss: 0.4425 - val_accuracy: 0.8374\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.8502 - val_loss: 0.4420 - val_accuracy: 0.8374\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4177 - accuracy: 0.8502 - val_loss: 0.4414 - val_accuracy: 0.8374\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8502 - val_loss: 0.4410 - val_accuracy: 0.8374\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.8502 - val_loss: 0.4405 - val_accuracy: 0.8374\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4161 - accuracy: 0.8502 - val_loss: 0.4400 - val_accuracy: 0.8374\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4156 - accuracy: 0.8502 - val_loss: 0.4396 - val_accuracy: 0.8374\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4150 - accuracy: 0.8502 - val_loss: 0.4391 - val_accuracy: 0.8374\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.8502 - val_loss: 0.4387 - val_accuracy: 0.8374\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4139 - accuracy: 0.8502 - val_loss: 0.4384 - val_accuracy: 0.8374\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4135 - accuracy: 0.8502 - val_loss: 0.4378 - val_accuracy: 0.8374\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8502 - val_loss: 0.4375 - val_accuracy: 0.8374\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4124 - accuracy: 0.8502 - val_loss: 0.4369 - val_accuracy: 0.8374\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4119 - accuracy: 0.8502 - val_loss: 0.4366 - val_accuracy: 0.8374\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4114 - accuracy: 0.8502 - val_loss: 0.4361 - val_accuracy: 0.8374\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4109 - accuracy: 0.8502 - val_loss: 0.4357 - val_accuracy: 0.8374\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4104 - accuracy: 0.8502 - val_loss: 0.4354 - val_accuracy: 0.8374\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.8502 - val_loss: 0.4350 - val_accuracy: 0.8374\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4095 - accuracy: 0.8502 - val_loss: 0.4347 - val_accuracy: 0.8374\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4090 - accuracy: 0.8502 - val_loss: 0.4343 - val_accuracy: 0.8374\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.8502 - val_loss: 0.4339 - val_accuracy: 0.8374\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4081 - accuracy: 0.8502 - val_loss: 0.4335 - val_accuracy: 0.8374\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8502 - val_loss: 0.4331 - val_accuracy: 0.8374\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8502 - val_loss: 0.4328 - val_accuracy: 0.8374\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4068 - accuracy: 0.8502 - val_loss: 0.4326 - val_accuracy: 0.8374\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8502 - val_loss: 0.4323 - val_accuracy: 0.8374\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4060 - accuracy: 0.8502 - val_loss: 0.4319 - val_accuracy: 0.8374\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4056 - accuracy: 0.8502 - val_loss: 0.4315 - val_accuracy: 0.8374\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.8502 - val_loss: 0.4311 - val_accuracy: 0.8374\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4048 - accuracy: 0.8502 - val_loss: 0.4308 - val_accuracy: 0.8374\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4044 - accuracy: 0.8502 - val_loss: 0.4306 - val_accuracy: 0.8374\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8502 - val_loss: 0.4299 - val_accuracy: 0.8374\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4038 - accuracy: 0.8502 - val_loss: 0.4297 - val_accuracy: 0.8374\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8502 - val_loss: 0.4295 - val_accuracy: 0.8374\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4031 - accuracy: 0.8502 - val_loss: 0.4294 - val_accuracy: 0.8374\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8502 - val_loss: 0.4292 - val_accuracy: 0.8374\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4025 - accuracy: 0.8502 - val_loss: 0.4288 - val_accuracy: 0.8374\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4022 - accuracy: 0.8502 - val_loss: 0.4284 - val_accuracy: 0.8374\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4019 - accuracy: 0.8502 - val_loss: 0.4281 - val_accuracy: 0.8374\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.8502 - val_loss: 0.4280 - val_accuracy: 0.8374\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4014 - accuracy: 0.8502 - val_loss: 0.4278 - val_accuracy: 0.8374\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8502 - val_loss: 0.4274 - val_accuracy: 0.8374\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4008 - accuracy: 0.8502 - val_loss: 0.4271 - val_accuracy: 0.8374\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8502 - val_loss: 0.4271 - val_accuracy: 0.8374\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.8502 - val_loss: 0.4270 - val_accuracy: 0.8374\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8502 - val_loss: 0.4267 - val_accuracy: 0.8374\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3999 - accuracy: 0.8502 - val_loss: 0.4267 - val_accuracy: 0.8374\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3997 - accuracy: 0.8502 - val_loss: 0.4262 - val_accuracy: 0.8374\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3995 - accuracy: 0.8502 - val_loss: 0.4260 - val_accuracy: 0.8374\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8502 - val_loss: 0.4259 - val_accuracy: 0.8374\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3991 - accuracy: 0.8502 - val_loss: 0.4258 - val_accuracy: 0.8374\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3989 - accuracy: 0.8502 - val_loss: 0.4256 - val_accuracy: 0.8374\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3987 - accuracy: 0.8502 - val_loss: 0.4254 - val_accuracy: 0.8374\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3986 - accuracy: 0.8502 - val_loss: 0.4251 - val_accuracy: 0.8374\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8502 - val_loss: 0.4253 - val_accuracy: 0.8374\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3983 - accuracy: 0.8502 - val_loss: 0.4251 - val_accuracy: 0.8374\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8502 - val_loss: 0.4246 - val_accuracy: 0.8374\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3978 - accuracy: 0.8502 - val_loss: 0.4252 - val_accuracy: 0.8374\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3979 - accuracy: 0.8502 - val_loss: 0.4248 - val_accuracy: 0.8374\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3976 - accuracy: 0.8502 - val_loss: 0.4249 - val_accuracy: 0.8374\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3976 - accuracy: 0.8502 - val_loss: 0.4245 - val_accuracy: 0.8374\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3975 - accuracy: 0.8502 - val_loss: 0.4241 - val_accuracy: 0.8374\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.8502 - val_loss: 0.4239 - val_accuracy: 0.8374\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3972 - accuracy: 0.8502 - val_loss: 0.4242 - val_accuracy: 0.8374\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3970 - accuracy: 0.8502 - val_loss: 0.4241 - val_accuracy: 0.8374\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3969 - accuracy: 0.8502 - val_loss: 0.4241 - val_accuracy: 0.8374\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3968 - accuracy: 0.8502 - val_loss: 0.4237 - val_accuracy: 0.8374\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3967 - accuracy: 0.8502 - val_loss: 0.4235 - val_accuracy: 0.8374\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3966 - accuracy: 0.8502 - val_loss: 0.4232 - val_accuracy: 0.8374\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3965 - accuracy: 0.8502 - val_loss: 0.4230 - val_accuracy: 0.8374\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3964 - accuracy: 0.8502 - val_loss: 0.4229 - val_accuracy: 0.8374\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3962 - accuracy: 0.8502 - val_loss: 0.4229 - val_accuracy: 0.8374\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3962 - accuracy: 0.8502 - val_loss: 0.4230 - val_accuracy: 0.8374\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3961 - accuracy: 0.8502 - val_loss: 0.4229 - val_accuracy: 0.8374\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8502 - val_loss: 0.4226 - val_accuracy: 0.8374\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3959 - accuracy: 0.8502 - val_loss: 0.4225 - val_accuracy: 0.8374\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8502 - val_loss: 0.4227 - val_accuracy: 0.8374\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8502 - val_loss: 0.4225 - val_accuracy: 0.8374\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8502 - val_loss: 0.4223 - val_accuracy: 0.8374\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8502 - val_loss: 0.4222 - val_accuracy: 0.8374\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8502 - val_loss: 0.4222 - val_accuracy: 0.8374\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3954 - accuracy: 0.8502 - val_loss: 0.4220 - val_accuracy: 0.8374\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8502 - val_loss: 0.4220 - val_accuracy: 0.8374\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3952 - accuracy: 0.8502 - val_loss: 0.4216 - val_accuracy: 0.8374\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3952 - accuracy: 0.8502 - val_loss: 0.4222 - val_accuracy: 0.8374\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3951 - accuracy: 0.8502 - val_loss: 0.4215 - val_accuracy: 0.8374\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3951 - accuracy: 0.8502 - val_loss: 0.4216 - val_accuracy: 0.8374\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8502 - val_loss: 0.4219 - val_accuracy: 0.8374\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8502 - val_loss: 0.4215 - val_accuracy: 0.8374\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.8502 - val_loss: 0.4213 - val_accuracy: 0.8374\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.8502 - val_loss: 0.4213 - val_accuracy: 0.8374\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8502 - val_loss: 0.4214 - val_accuracy: 0.8374\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8502 - val_loss: 0.4213 - val_accuracy: 0.8374\n"
     ]
    }
   ],
   "source": [
    "mlp = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(4, input_dim = 5, activation = 'tanh'),\n",
    "      tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "mlp.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "## Building the model\n",
    "md_mlp = mlp.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "                 epochs = 100,\n",
    "                 batch_size = 32, \n",
    "                 validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8679b9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15323302, 0.06984502, 0.13597395, 0.2146586 , 0.1560349 ,\n",
       "       0.07139845, 0.07139841, 0.31990385, 0.05781847, 0.30887046,\n",
       "       0.21517548, 0.10266523, 0.1052977 , 0.17609064, 0.10253438,\n",
       "       0.0804897 , 0.1426054 , 0.1391601 , 0.05057511, 0.08691587,\n",
       "       0.06458239, 0.10722248, 0.0787853 , 0.30917954, 0.07386761,\n",
       "       0.1006656 , 0.10923968, 0.21782225, 0.07541741, 0.0901816 ,\n",
       "       0.08316138, 0.11798892, 0.14070423, 0.1586984 , 0.21846287,\n",
       "       0.15870616, 0.17795467, 0.08347883, 0.25167057, 0.16057757,\n",
       "       0.26159593, 0.11303103, 0.0910034 , 0.08102007, 0.06173062,\n",
       "       0.30791318, 0.14463773, 0.07282171, 0.21666066, 0.20883271,\n",
       "       0.08741899, 0.18231405, 0.1779998 , 0.1743781 , 0.17675932,\n",
       "       0.2107315 , 0.10983691, 0.18581906, 0.3011414 , 0.10917295,\n",
       "       0.13679817, 0.10683965, 0.24742593, 0.0987094 , 0.32997146,\n",
       "       0.0880011 , 0.17003399, 0.11081491, 0.06492715, 0.13743582,\n",
       "       0.26303503, 0.16964886, 0.06806266, 0.04839081, 0.06057626,\n",
       "       0.16030592, 0.13830422, 0.06557975, 0.24769959, 0.06989257,\n",
       "       0.10290067, 0.26697075, 0.03868598, 0.04487929, 0.25899047,\n",
       "       0.09940562, 0.15594676, 0.07447845, 0.07962967, 0.42510346,\n",
       "       0.06055994, 0.2773301 , 0.1239589 , 0.13578282, 0.0633575 ,\n",
       "       0.12335557, 0.25999194, 0.07034978, 0.09363037, 0.1853793 ,\n",
       "       0.21466908, 0.18558004, 0.1046758 , 0.08746391, 0.26401073,\n",
       "       0.1187819 , 0.22724749, 0.09320384, 0.17573732, 0.12585051,\n",
       "       0.1744392 , 0.14818577, 0.12364619, 0.07917378, 0.10602971,\n",
       "       0.13254835, 0.21370257, 0.1244022 , 0.1603048 , 0.08062808,\n",
       "       0.21573578, 0.05597295, 0.05618502, 0.09290682, 0.10737941,\n",
       "       0.21499857, 0.18090549, 0.1920294 , 0.18086618, 0.19533256,\n",
       "       0.06341324, 0.07570174, 0.09229071, 0.1683673 , 0.20403692,\n",
       "       0.08198416, 0.12584828, 0.19985002, 0.08384389, 0.09573581,\n",
       "       0.26552463, 0.36933804, 0.07374511, 0.13555424, 0.07836428,\n",
       "       0.20481865, 0.15599291, 0.0965993 , 0.26611394, 0.15513231,\n",
       "       0.15796895, 0.16421317, 0.06350153, 0.23474702, 0.13204505,\n",
       "       0.23391896, 0.23033448, 0.31141123, 0.3028761 , 0.20430928,\n",
       "       0.11133043, 0.08533077, 0.12861976, 0.29789075, 0.17467938,\n",
       "       0.18744133, 0.19725208, 0.08597869, 0.22757068, 0.05911585,\n",
       "       0.11902713, 0.05373533, 0.06888288, 0.0655344 , 0.11264123,\n",
       "       0.0833854 , 0.19327748, 0.04465984, 0.08867647, 0.09437679,\n",
       "       0.12746644, 0.10379701, 0.07929331, 0.07059804, 0.13362768,\n",
       "       0.07041991, 0.13835488, 0.07491338, 0.07240239, 0.18918069,\n",
       "       0.1811148 , 0.09359144, 0.30161572, 0.2319007 , 0.14579186,\n",
       "       0.05767756, 0.1881013 , 0.05793779, 0.08453178, 0.16773294,\n",
       "       0.38488474, 0.20278981, 0.19031015, 0.06196581, 0.15421452,\n",
       "       0.08927643, 0.19783999, 0.1442631 , 0.151855  , 0.13401125,\n",
       "       0.18593441, 0.04622347, 0.16831575, 0.3954651 , 0.09615326,\n",
       "       0.10875146, 0.21066657, 0.2539152 , 0.1198431 , 0.16390608,\n",
       "       0.22141504, 0.10361481, 0.17971322, 0.17447434, 0.31826937,\n",
       "       0.11047882, 0.05108324, 0.10701375, 0.14252023, 0.11272914,\n",
       "       0.21343353, 0.21521479, 0.17581138, 0.10898621, 0.14484206,\n",
       "       0.1274414 , 0.19308811, 0.18732509, 0.14283903, 0.10662249,\n",
       "       0.15863338, 0.15397964, 0.23915592, 0.11153334, 0.15693508,\n",
       "       0.14682917, 0.15091403, 0.1252916 , 0.24962421, 0.22019662,\n",
       "       0.08092507, 0.13870685, 0.11150564, 0.2918991 , 0.07915605,\n",
       "       0.1467446 , 0.31795546, 0.09787638, 0.08689715, 0.05141222,\n",
       "       0.06868022, 0.18138707, 0.1370166 , 0.13766465, 0.09661289,\n",
       "       0.06644977, 0.16401334, 0.10143099, 0.11141712, 0.09513331,\n",
       "       0.11170443, 0.09667498, 0.14499478, 0.13083902, 0.04970596,\n",
       "       0.09271666, 0.14071698, 0.2508475 , 0.08187698, 0.11095694,\n",
       "       0.20847121, 0.1340986 , 0.10519923, 0.15473829, 0.0919564 ,\n",
       "       0.09719492, 0.07400046, 0.14942665, 0.18823314, 0.27360573,\n",
       "       0.04532124, 0.23716623, 0.11832504, 0.18121283, 0.08585675,\n",
       "       0.12032946, 0.2048599 , 0.18026836, 0.04064543, 0.21634597,\n",
       "       0.10475626, 0.23601085, 0.249179  , 0.11675256, 0.1493324 ,\n",
       "       0.10284593, 0.08790556, 0.14192188, 0.05699385, 0.06036449,\n",
       "       0.10459163, 0.12893546, 0.09165043, 0.22135255, 0.0955395 ,\n",
       "       0.24614255, 0.12752023, 0.10404629, 0.11043695, 0.06606948,\n",
       "       0.07322074, 0.19616824, 0.17106573, 0.2290892 , 0.28240398,\n",
       "       0.16989109, 0.15754128, 0.14682834, 0.28551722, 0.06946576,\n",
       "       0.13568637, 0.20926291, 0.07840537, 0.05714308, 0.1613037 ,\n",
       "       0.08960382, 0.1339622 , 0.13050652, 0.08784208, 0.27151525,\n",
       "       0.27416244, 0.06587323, 0.26206353, 0.08137051, 0.20971191,\n",
       "       0.07906336, 0.18992203, 0.04324665, 0.11585239, 0.0830581 ,\n",
       "       0.11832965, 0.1977065 , 0.09258715, 0.37643465, 0.21464008,\n",
       "       0.08397368, 0.1655615 , 0.05240983, 0.23556828, 0.35774928,\n",
       "       0.13708512, 0.14969304, 0.07375306, 0.07432827, 0.14415535,\n",
       "       0.23037918, 0.07767535, 0.06308337, 0.14440994, 0.16278265,\n",
       "       0.13683617, 0.13005689, 0.1669539 , 0.09365302, 0.16437861,\n",
       "       0.209105  , 0.08479082, 0.20654725, 0.25599214, 0.12561287,\n",
       "       0.09200247, 0.10545965, 0.04633975, 0.13277218, 0.06145365,\n",
       "       0.14787601, 0.23083548, 0.14028038, 0.103487  , 0.06390011,\n",
       "       0.1508192 , 0.22303541, 0.10557894, 0.28724083, 0.1092144 ,\n",
       "       0.13752864, 0.16652793, 0.24969122, 0.33739465, 0.17301697,\n",
       "       0.13939838, 0.19378813, 0.13263774, 0.07991318, 0.10769155,\n",
       "       0.2940468 , 0.09595986, 0.09156776, 0.33715245, 0.1885965 ,\n",
       "       0.18496759, 0.1512866 , 0.1035305 , 0.084145  , 0.0555925 ,\n",
       "       0.11286485, 0.13738632, 0.16489604, 0.187428  , 0.20905504,\n",
       "       0.24868338, 0.17220536, 0.15153094, 0.13128188, 0.2353523 ,\n",
       "       0.13422325, 0.2663582 , 0.04217422, 0.19859582, 0.24658845,\n",
       "       0.09690781, 0.18550898, 0.1736057 , 0.29776564, 0.25162268,\n",
       "       0.40611544, 0.20205024, 0.14569992, 0.17076786, 0.18147981,\n",
       "       0.15567803, 0.05584165, 0.07760424, 0.10891742, 0.04375053,\n",
       "       0.31696904, 0.19946447, 0.18481775, 0.1451805 , 0.2584018 ,\n",
       "       0.20258874, 0.11869831, 0.13659574, 0.17193012, 0.06180535,\n",
       "       0.05667599, 0.30037823, 0.21336263, 0.07897991, 0.06851457,\n",
       "       0.05957834, 0.21355487, 0.32012773, 0.06510562, 0.2013894 ,\n",
       "       0.21656889, 0.10792842, 0.114819  , 0.21006766, 0.09713028,\n",
       "       0.17138477, 0.27172995, 0.15461536, 0.3030547 , 0.10605676,\n",
       "       0.28634736, 0.11301529, 0.09721907, 0.12765093, 0.21074384,\n",
       "       0.15246503, 0.10434391, 0.05173187, 0.06364381, 0.05403917,\n",
       "       0.0751284 , 0.11361361, 0.12216745, 0.1340844 , 0.09079851,\n",
       "       0.1600892 , 0.06566536, 0.1234521 , 0.180933  , 0.05503432,\n",
       "       0.22026557, 0.08572293, 0.09312977, 0.16308717, 0.09105745,\n",
       "       0.12019455, 0.16737626, 0.21238546, 0.08095255, 0.16243596,\n",
       "       0.2317505 , 0.04983025, 0.2706064 , 0.10032209, 0.12228361,\n",
       "       0.14477053, 0.25821048, 0.09335846, 0.12123667, 0.09815523,\n",
       "       0.11763416, 0.13821118, 0.11017218, 0.04048746, 0.06734776,\n",
       "       0.16003275, 0.25523046, 0.18297043, 0.2596288 , 0.08439531,\n",
       "       0.21561831, 0.09077899, 0.12608495, 0.20696585, 0.17665985,\n",
       "       0.07878572, 0.30618486, 0.0890564 , 0.05226821, 0.10570017,\n",
       "       0.1891403 , 0.10155378, 0.05439346, 0.34493023, 0.08845815,\n",
       "       0.14732613, 0.06658641, 0.21099988, 0.15859655, 0.23752497,\n",
       "       0.06214578, 0.31103355, 0.09652733, 0.2117662 , 0.17855597,\n",
       "       0.08844779, 0.07181847, 0.10102544, 0.29672986, 0.23903541,\n",
       "       0.17654479, 0.05666706, 0.05102313, 0.07929543, 0.05066892,\n",
       "       0.11582229, 0.09245434, 0.07815879, 0.14916667, 0.17754951,\n",
       "       0.06836188, 0.09672879, 0.05337345, 0.11410036, 0.25295153,\n",
       "       0.15986349, 0.13138929, 0.14098595, 0.28679636, 0.10224245,\n",
       "       0.16426876, 0.22038436, 0.1005887 , 0.11574642, 0.25223362,\n",
       "       0.14854814, 0.17589535, 0.13904312, 0.10445061, 0.05790222,\n",
       "       0.23454732, 0.06055496, 0.10403021, 0.2387226 , 0.13449663,\n",
       "       0.05575162, 0.13301048, 0.20372179, 0.19481437, 0.2648094 ,\n",
       "       0.09102038, 0.1094612 , 0.16206549, 0.06813669, 0.12019888,\n",
       "       0.3160553 , 0.08652096, 0.25828263, 0.29460448, 0.09238982,\n",
       "       0.07942921, 0.0696201 , 0.09412383, 0.12212668, 0.10466146,\n",
       "       0.2659606 , 0.15898347, 0.26590753, 0.2284513 , 0.10852826,\n",
       "       0.07428775, 0.21788917, 0.18593489, 0.2936523 , 0.17195626,\n",
       "       0.08227129, 0.25257105, 0.1115034 , 0.29536122, 0.04910312,\n",
       "       0.16194077, 0.3230038 , 0.13677397, 0.07587082, 0.09816367,\n",
       "       0.13026549, 0.16337912, 0.11905658, 0.27401915, 0.13507031,\n",
       "       0.22984284, 0.12249602, 0.19481003, 0.19701385, 0.07926314,\n",
       "       0.17439362, 0.09207443, 0.10248987, 0.19699132, 0.07232202,\n",
       "       0.10620436, 0.1959887 , 0.11415686, 0.09248188, 0.05195213,\n",
       "       0.10146133, 0.07969639, 0.14025486, 0.12031448, 0.08911348,\n",
       "       0.30819684, 0.09079923, 0.07996129, 0.13178506, 0.15579754,\n",
       "       0.13489991, 0.06482688, 0.08482676, 0.20270805, 0.0887202 ,\n",
       "       0.16424787, 0.11384169, 0.05893458, 0.07984435, 0.16205221,\n",
       "       0.0754825 , 0.11506846, 0.07161136, 0.17575492, 0.09920523,\n",
       "       0.14515296, 0.13117908, 0.3220194 , 0.1520403 , 0.23701589,\n",
       "       0.19971709, 0.13600998, 0.06618565, 0.10314503, 0.07456744,\n",
       "       0.1723361 , 0.14018305, 0.13616407, 0.3724521 , 0.2966387 ,\n",
       "       0.2812159 , 0.20086831, 0.08865106, 0.20866702, 0.12691037,\n",
       "       0.10585194, 0.1967787 , 0.17579229, 0.12700407, 0.12857243,\n",
       "       0.20299026, 0.10218476, 0.1649001 , 0.29569787, 0.38514316,\n",
       "       0.16053452, 0.11727869, 0.10516374, 0.08433894, 0.08247303,\n",
       "       0.0710062 , 0.19515558, 0.10402821, 0.07416543, 0.08627681,\n",
       "       0.08289739, 0.25338683, 0.23692119, 0.15676257, 0.11534982,\n",
       "       0.05265888, 0.04394629, 0.09100842, 0.1847542 , 0.08232528,\n",
       "       0.25786355, 0.10887804, 0.1605586 , 0.2737542 , 0.23276788,\n",
       "       0.22229384, 0.20012023], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extracting predictions\n",
    "mlp_pred = mlp.predict(X_test)[:, 1]\n",
    "mlp_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
