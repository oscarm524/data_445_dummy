{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d95a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.3 MB 18 kB/s s eta 0:00:01     |████████████▊                   | 182.6 MB 75.2 MB/s eta 0:00:04��█████████████▎              | 247.1 MB 79.1 MB/s eta 0:00:03��█████████████▌              | 251.0 MB 79.1 MB/s eta 0:00:03     |██████████████████████████████▎ | 434.0 MB 66.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 61.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.15.2)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 61.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.40.0-cp36-cp36m-manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 61.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.14.0-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 81.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 274 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 64.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (49.6.0.post20210108)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 82.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 81.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 64.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 82.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 82.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.0)\n",
      "Building wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=e4dbea4afb43492be2191faca98759c1bae493d606f78c01c83b3d0ed17e2286\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=cd55aa9a5906c508e34468995beceec013e562cb333142f9e2fcd6ea2f907ae7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.14.0 astunparse-1.6.3 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 grpcio-1.40.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7cc909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "\n",
    "## Defining the bucket \n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-445'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the csv file \n",
    "file_key = 'Fall_2021/In_Class_Assignments/framingham.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "## Reading the csv file\n",
    "heart = pd.read_csv(file_content_stream)\n",
    "heart = heart.dropna()\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3c1e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "X = heart[['age', 'currentSmoker', 'totChol', 'BMI', 'heartRate']]\n",
    "Y = heart['TenYearCHD']\n",
    "\n",
    "## Splitting the data \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ff1ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming the input variables\n",
    "scaler_train = MinMaxScaler(feature_range = (0, 1)).fit(X_train)\n",
    "X_train = scaler_train.transform(X_train)\n",
    "\n",
    "scaler_test = MinMaxScaler(feature_range = (0, 1)).fit(X_test)\n",
    "X_test = scaler_train.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a351bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.7534 - accuracy: 0.1631 - val_loss: 0.7295 - val_accuracy: 0.2117\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.7187 - accuracy: 0.2774 - val_loss: 0.6986 - val_accuracy: 0.4508\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.5557 - val_loss: 0.6715 - val_accuracy: 0.7473\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6615 - accuracy: 0.7900 - val_loss: 0.6477 - val_accuracy: 0.8197\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.6379 - accuracy: 0.8413 - val_loss: 0.6266 - val_accuracy: 0.8333\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6170 - accuracy: 0.8499 - val_loss: 0.6080 - val_accuracy: 0.8374\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5984 - accuracy: 0.8505 - val_loss: 0.5916 - val_accuracy: 0.8374\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5820 - accuracy: 0.8502 - val_loss: 0.5770 - val_accuracy: 0.8374\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5673 - accuracy: 0.8502 - val_loss: 0.5641 - val_accuracy: 0.8374\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5543 - accuracy: 0.8502 - val_loss: 0.5527 - val_accuracy: 0.8374\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5427 - accuracy: 0.8502 - val_loss: 0.5425 - val_accuracy: 0.8374\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5323 - accuracy: 0.8502 - val_loss: 0.5334 - val_accuracy: 0.8374\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5230 - accuracy: 0.8502 - val_loss: 0.5253 - val_accuracy: 0.8374\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5147 - accuracy: 0.8502 - val_loss: 0.5180 - val_accuracy: 0.8374\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5072 - accuracy: 0.8502 - val_loss: 0.5115 - val_accuracy: 0.8374\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5004 - accuracy: 0.8502 - val_loss: 0.5057 - val_accuracy: 0.8374\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4944 - accuracy: 0.8502 - val_loss: 0.5004 - val_accuracy: 0.8374\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4889 - accuracy: 0.8502 - val_loss: 0.4957 - val_accuracy: 0.8374\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4839 - accuracy: 0.8502 - val_loss: 0.4915 - val_accuracy: 0.8374\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4794 - accuracy: 0.8502 - val_loss: 0.4877 - val_accuracy: 0.8374\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4753 - accuracy: 0.8502 - val_loss: 0.4842 - val_accuracy: 0.8374\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4716 - accuracy: 0.8502 - val_loss: 0.4811 - val_accuracy: 0.8374\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4683 - accuracy: 0.8502 - val_loss: 0.4783 - val_accuracy: 0.8374\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4652 - accuracy: 0.8502 - val_loss: 0.4758 - val_accuracy: 0.8374\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4625 - accuracy: 0.8502 - val_loss: 0.4735 - val_accuracy: 0.8374\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4599 - accuracy: 0.8502 - val_loss: 0.4714 - val_accuracy: 0.8374\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4576 - accuracy: 0.8502 - val_loss: 0.4695 - val_accuracy: 0.8374\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4555 - accuracy: 0.8502 - val_loss: 0.4678 - val_accuracy: 0.8374\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4536 - accuracy: 0.8502 - val_loss: 0.4662 - val_accuracy: 0.8374\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4518 - accuracy: 0.8502 - val_loss: 0.4648 - val_accuracy: 0.8374\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4502 - accuracy: 0.8502 - val_loss: 0.4635 - val_accuracy: 0.8374\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.8502 - val_loss: 0.4623 - val_accuracy: 0.8374\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4473 - accuracy: 0.8502 - val_loss: 0.4613 - val_accuracy: 0.8374\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4460 - accuracy: 0.8502 - val_loss: 0.4603 - val_accuracy: 0.8374\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4449 - accuracy: 0.8502 - val_loss: 0.4594 - val_accuracy: 0.8374\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4438 - accuracy: 0.8502 - val_loss: 0.4586 - val_accuracy: 0.8374\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4429 - accuracy: 0.8502 - val_loss: 0.4579 - val_accuracy: 0.8374\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4419 - accuracy: 0.8502 - val_loss: 0.4572 - val_accuracy: 0.8374\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4411 - accuracy: 0.8502 - val_loss: 0.4566 - val_accuracy: 0.8374\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4403 - accuracy: 0.8502 - val_loss: 0.4560 - val_accuracy: 0.8374\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4396 - accuracy: 0.8502 - val_loss: 0.4555 - val_accuracy: 0.8374\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.8502 - val_loss: 0.4550 - val_accuracy: 0.8374\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4383 - accuracy: 0.8502 - val_loss: 0.4546 - val_accuracy: 0.8374\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4377 - accuracy: 0.8502 - val_loss: 0.4542 - val_accuracy: 0.8374\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4372 - accuracy: 0.8502 - val_loss: 0.4538 - val_accuracy: 0.8374\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4367 - accuracy: 0.8502 - val_loss: 0.4535 - val_accuracy: 0.8374\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4363 - accuracy: 0.8502 - val_loss: 0.4532 - val_accuracy: 0.8374\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4358 - accuracy: 0.8502 - val_loss: 0.4529 - val_accuracy: 0.8374\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4355 - accuracy: 0.8502 - val_loss: 0.4527 - val_accuracy: 0.8374\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4351 - accuracy: 0.8502 - val_loss: 0.4524 - val_accuracy: 0.8374\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4347 - accuracy: 0.8502 - val_loss: 0.4522 - val_accuracy: 0.8374\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.8502 - val_loss: 0.4520 - val_accuracy: 0.8374\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4341 - accuracy: 0.8502 - val_loss: 0.4518 - val_accuracy: 0.8374\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4338 - accuracy: 0.8502 - val_loss: 0.4516 - val_accuracy: 0.8374\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4336 - accuracy: 0.8502 - val_loss: 0.4515 - val_accuracy: 0.8374\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4333 - accuracy: 0.8502 - val_loss: 0.4513 - val_accuracy: 0.8374\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4331 - accuracy: 0.8502 - val_loss: 0.4512 - val_accuracy: 0.8374\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4328 - accuracy: 0.8502 - val_loss: 0.4511 - val_accuracy: 0.8374\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4326 - accuracy: 0.8502 - val_loss: 0.4510 - val_accuracy: 0.8374\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4324 - accuracy: 0.8502 - val_loss: 0.4509 - val_accuracy: 0.8374\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4322 - accuracy: 0.8502 - val_loss: 0.4508 - val_accuracy: 0.8374\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4321 - accuracy: 0.8502 - val_loss: 0.4507 - val_accuracy: 0.8374\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4319 - accuracy: 0.8502 - val_loss: 0.4506 - val_accuracy: 0.8374\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4317 - accuracy: 0.8502 - val_loss: 0.4505 - val_accuracy: 0.8374\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4316 - accuracy: 0.8502 - val_loss: 0.4504 - val_accuracy: 0.8374\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4315 - accuracy: 0.8502 - val_loss: 0.4504 - val_accuracy: 0.8374\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4313 - accuracy: 0.8502 - val_loss: 0.4503 - val_accuracy: 0.8374\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4312 - accuracy: 0.8502 - val_loss: 0.4502 - val_accuracy: 0.8374\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4311 - accuracy: 0.8502 - val_loss: 0.4502 - val_accuracy: 0.8374\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4310 - accuracy: 0.8502 - val_loss: 0.4501 - val_accuracy: 0.8374\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4308 - accuracy: 0.8502 - val_loss: 0.4501 - val_accuracy: 0.8374\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4307 - accuracy: 0.8502 - val_loss: 0.4500 - val_accuracy: 0.8374\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4306 - accuracy: 0.8502 - val_loss: 0.4500 - val_accuracy: 0.8374\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4305 - accuracy: 0.8502 - val_loss: 0.4499 - val_accuracy: 0.8374\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4304 - accuracy: 0.8502 - val_loss: 0.4499 - val_accuracy: 0.8374\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4303 - accuracy: 0.8502 - val_loss: 0.4499 - val_accuracy: 0.8374\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4303 - accuracy: 0.8502 - val_loss: 0.4498 - val_accuracy: 0.8374\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.8502 - val_loss: 0.4498 - val_accuracy: 0.8374\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4301 - accuracy: 0.8502 - val_loss: 0.4498 - val_accuracy: 0.8374\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4300 - accuracy: 0.8502 - val_loss: 0.4497 - val_accuracy: 0.8374\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4299 - accuracy: 0.8502 - val_loss: 0.4497 - val_accuracy: 0.8374\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4299 - accuracy: 0.8502 - val_loss: 0.4496 - val_accuracy: 0.8374\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4298 - accuracy: 0.8502 - val_loss: 0.4496 - val_accuracy: 0.8374\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4297 - accuracy: 0.8502 - val_loss: 0.4496 - val_accuracy: 0.8374\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.8502 - val_loss: 0.4496 - val_accuracy: 0.8374\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.8502 - val_loss: 0.4495 - val_accuracy: 0.8374\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.8502 - val_loss: 0.4495 - val_accuracy: 0.8374\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4294 - accuracy: 0.8502 - val_loss: 0.4495 - val_accuracy: 0.8374\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4294 - accuracy: 0.8502 - val_loss: 0.4494 - val_accuracy: 0.8374\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4293 - accuracy: 0.8502 - val_loss: 0.4494 - val_accuracy: 0.8374\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4293 - accuracy: 0.8502 - val_loss: 0.4494 - val_accuracy: 0.8374\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4292 - accuracy: 0.8502 - val_loss: 0.4494 - val_accuracy: 0.8374\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4291 - accuracy: 0.8502 - val_loss: 0.4493 - val_accuracy: 0.8374\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4291 - accuracy: 0.8502 - val_loss: 0.4493 - val_accuracy: 0.8374\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4290 - accuracy: 0.8502 - val_loss: 0.4493 - val_accuracy: 0.8374\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4290 - accuracy: 0.8502 - val_loss: 0.4492 - val_accuracy: 0.8374\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4289 - accuracy: 0.8502 - val_loss: 0.4492 - val_accuracy: 0.8374\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4289 - accuracy: 0.8502 - val_loss: 0.4492 - val_accuracy: 0.8374\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4288 - accuracy: 0.8502 - val_loss: 0.4492 - val_accuracy: 0.8374\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4287 - accuracy: 0.8502 - val_loss: 0.4491 - val_accuracy: 0.8374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc534069c18>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1 = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(4, input_dim = 5, activation = 'tanh'),\n",
    "      tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "mlp1.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "## Building the model\n",
    "mlp1.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "         epochs = 100,\n",
    "         batch_size = 500, \n",
    "         validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a3a762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.6409 - accuracy: 0.7541 - val_loss: 0.6356 - val_accuracy: 0.8005\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6246 - accuracy: 0.8393 - val_loss: 0.6204 - val_accuracy: 0.8374\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6092 - accuracy: 0.8502 - val_loss: 0.6062 - val_accuracy: 0.8374\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5947 - accuracy: 0.8502 - val_loss: 0.5927 - val_accuracy: 0.8374\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5810 - accuracy: 0.8502 - val_loss: 0.5802 - val_accuracy: 0.8374\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5682 - accuracy: 0.8502 - val_loss: 0.5684 - val_accuracy: 0.8374\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5562 - accuracy: 0.8502 - val_loss: 0.5576 - val_accuracy: 0.8374\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5451 - accuracy: 0.8502 - val_loss: 0.5475 - val_accuracy: 0.8374\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5347 - accuracy: 0.8502 - val_loss: 0.5383 - val_accuracy: 0.8374\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5253 - accuracy: 0.8502 - val_loss: 0.5298 - val_accuracy: 0.8374\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5166 - accuracy: 0.8502 - val_loss: 0.5220 - val_accuracy: 0.8374\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5086 - accuracy: 0.8502 - val_loss: 0.5149 - val_accuracy: 0.8374\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5014 - accuracy: 0.8502 - val_loss: 0.5084 - val_accuracy: 0.8374\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4947 - accuracy: 0.8502 - val_loss: 0.5025 - val_accuracy: 0.8374\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4887 - accuracy: 0.8502 - val_loss: 0.4972 - val_accuracy: 0.8374\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4833 - accuracy: 0.8502 - val_loss: 0.4923 - val_accuracy: 0.8374\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4783 - accuracy: 0.8502 - val_loss: 0.4880 - val_accuracy: 0.8374\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4738 - accuracy: 0.8502 - val_loss: 0.4842 - val_accuracy: 0.8374\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4697 - accuracy: 0.8502 - val_loss: 0.4806 - val_accuracy: 0.8374\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4660 - accuracy: 0.8502 - val_loss: 0.4774 - val_accuracy: 0.8374\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4627 - accuracy: 0.8502 - val_loss: 0.4746 - val_accuracy: 0.8374\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4596 - accuracy: 0.8502 - val_loss: 0.4720 - val_accuracy: 0.8374\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4569 - accuracy: 0.8502 - val_loss: 0.4696 - val_accuracy: 0.8374\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4544 - accuracy: 0.8502 - val_loss: 0.4675 - val_accuracy: 0.8374\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4521 - accuracy: 0.8502 - val_loss: 0.4656 - val_accuracy: 0.8374\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4500 - accuracy: 0.8502 - val_loss: 0.4639 - val_accuracy: 0.8374\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4481 - accuracy: 0.8502 - val_loss: 0.4623 - val_accuracy: 0.8374\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4464 - accuracy: 0.8502 - val_loss: 0.4609 - val_accuracy: 0.8374\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4448 - accuracy: 0.8502 - val_loss: 0.4596 - val_accuracy: 0.8374\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4434 - accuracy: 0.8502 - val_loss: 0.4585 - val_accuracy: 0.8374\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.8502 - val_loss: 0.4574 - val_accuracy: 0.8374\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4410 - accuracy: 0.8502 - val_loss: 0.4565 - val_accuracy: 0.8374\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4399 - accuracy: 0.8502 - val_loss: 0.4556 - val_accuracy: 0.8374\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.8502 - val_loss: 0.4549 - val_accuracy: 0.8374\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4380 - accuracy: 0.8502 - val_loss: 0.4542 - val_accuracy: 0.8374\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4372 - accuracy: 0.8502 - val_loss: 0.4536 - val_accuracy: 0.8374\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4364 - accuracy: 0.8502 - val_loss: 0.4530 - val_accuracy: 0.8374\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4358 - accuracy: 0.8502 - val_loss: 0.4525 - val_accuracy: 0.8374\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4351 - accuracy: 0.8502 - val_loss: 0.4520 - val_accuracy: 0.8374\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.8502 - val_loss: 0.4516 - val_accuracy: 0.8374\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4340 - accuracy: 0.8502 - val_loss: 0.4512 - val_accuracy: 0.8374\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4335 - accuracy: 0.8502 - val_loss: 0.4509 - val_accuracy: 0.8374\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4330 - accuracy: 0.8502 - val_loss: 0.4505 - val_accuracy: 0.8374\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4326 - accuracy: 0.8502 - val_loss: 0.4503 - val_accuracy: 0.8374\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.8502 - val_loss: 0.4500 - val_accuracy: 0.8374\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4319 - accuracy: 0.8502 - val_loss: 0.4498 - val_accuracy: 0.8374\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4315 - accuracy: 0.8502 - val_loss: 0.4495 - val_accuracy: 0.8374\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4313 - accuracy: 0.8502 - val_loss: 0.4493 - val_accuracy: 0.8374\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4310 - accuracy: 0.8502 - val_loss: 0.4492 - val_accuracy: 0.8374\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4307 - accuracy: 0.8502 - val_loss: 0.4490 - val_accuracy: 0.8374\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4305 - accuracy: 0.8502 - val_loss: 0.4488 - val_accuracy: 0.8374\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4302 - accuracy: 0.8502 - val_loss: 0.4487 - val_accuracy: 0.8374\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4300 - accuracy: 0.8502 - val_loss: 0.4486 - val_accuracy: 0.8374\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4298 - accuracy: 0.8502 - val_loss: 0.4485 - val_accuracy: 0.8374\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4296 - accuracy: 0.8502 - val_loss: 0.4484 - val_accuracy: 0.8374\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.8502 - val_loss: 0.4482 - val_accuracy: 0.8374\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4293 - accuracy: 0.8502 - val_loss: 0.4482 - val_accuracy: 0.8374\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4291 - accuracy: 0.8502 - val_loss: 0.4481 - val_accuracy: 0.8374\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4290 - accuracy: 0.8502 - val_loss: 0.4480 - val_accuracy: 0.8374\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4289 - accuracy: 0.8502 - val_loss: 0.4479 - val_accuracy: 0.8374\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4287 - accuracy: 0.8502 - val_loss: 0.4478 - val_accuracy: 0.8374\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4286 - accuracy: 0.8502 - val_loss: 0.4478 - val_accuracy: 0.8374\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4285 - accuracy: 0.8502 - val_loss: 0.4477 - val_accuracy: 0.8374\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.8502 - val_loss: 0.4477 - val_accuracy: 0.8374\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4283 - accuracy: 0.8502 - val_loss: 0.4476 - val_accuracy: 0.8374\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4282 - accuracy: 0.8502 - val_loss: 0.4476 - val_accuracy: 0.8374\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4281 - accuracy: 0.8502 - val_loss: 0.4475 - val_accuracy: 0.8374\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.8502 - val_loss: 0.4474 - val_accuracy: 0.8374\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4279 - accuracy: 0.8502 - val_loss: 0.4474 - val_accuracy: 0.8374\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4278 - accuracy: 0.8502 - val_loss: 0.4474 - val_accuracy: 0.8374\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4278 - accuracy: 0.8502 - val_loss: 0.4473 - val_accuracy: 0.8374\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4277 - accuracy: 0.8502 - val_loss: 0.4473 - val_accuracy: 0.8374\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4276 - accuracy: 0.8502 - val_loss: 0.4472 - val_accuracy: 0.8374\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4275 - accuracy: 0.8502 - val_loss: 0.4472 - val_accuracy: 0.8374\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4274 - accuracy: 0.8502 - val_loss: 0.4471 - val_accuracy: 0.8374\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4274 - accuracy: 0.8502 - val_loss: 0.4471 - val_accuracy: 0.8374\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4273 - accuracy: 0.8502 - val_loss: 0.4471 - val_accuracy: 0.8374\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4272 - accuracy: 0.8502 - val_loss: 0.4470 - val_accuracy: 0.8374\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4272 - accuracy: 0.8502 - val_loss: 0.4470 - val_accuracy: 0.8374\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4271 - accuracy: 0.8502 - val_loss: 0.4470 - val_accuracy: 0.8374\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4270 - accuracy: 0.8502 - val_loss: 0.4469 - val_accuracy: 0.8374\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4270 - accuracy: 0.8502 - val_loss: 0.4469 - val_accuracy: 0.8374\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4269 - accuracy: 0.8502 - val_loss: 0.4468 - val_accuracy: 0.8374\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4269 - accuracy: 0.8502 - val_loss: 0.4468 - val_accuracy: 0.8374\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4268 - accuracy: 0.8502 - val_loss: 0.4468 - val_accuracy: 0.8374\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4267 - accuracy: 0.8502 - val_loss: 0.4467 - val_accuracy: 0.8374\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4267 - accuracy: 0.8502 - val_loss: 0.4467 - val_accuracy: 0.8374\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4266 - accuracy: 0.8502 - val_loss: 0.4467 - val_accuracy: 0.8374\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4266 - accuracy: 0.8502 - val_loss: 0.4466 - val_accuracy: 0.8374\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4265 - accuracy: 0.8502 - val_loss: 0.4466 - val_accuracy: 0.8374\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.8502 - val_loss: 0.4466 - val_accuracy: 0.8374\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4264 - accuracy: 0.8502 - val_loss: 0.4465 - val_accuracy: 0.8374\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4264 - accuracy: 0.8502 - val_loss: 0.4465 - val_accuracy: 0.8374\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4263 - accuracy: 0.8502 - val_loss: 0.4465 - val_accuracy: 0.8374\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4263 - accuracy: 0.8502 - val_loss: 0.4464 - val_accuracy: 0.8374\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4262 - accuracy: 0.8502 - val_loss: 0.4464 - val_accuracy: 0.8374\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4262 - accuracy: 0.8502 - val_loss: 0.4463 - val_accuracy: 0.8374\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4261 - accuracy: 0.8502 - val_loss: 0.4463 - val_accuracy: 0.8374\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4261 - accuracy: 0.8502 - val_loss: 0.4463 - val_accuracy: 0.8374\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8502 - val_loss: 0.4462 - val_accuracy: 0.8374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5282eaf60>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2 = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(4, input_dim = 5, activation = 'relu'),\n",
    "      tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "mlp2.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "## Building the model\n",
    "mlp2.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "         epochs = 100,\n",
    "         batch_size = 500, \n",
    "         validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d6b4c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15323302, 0.06984502, 0.13597395, 0.2146586 , 0.1560349 ,\n",
       "       0.07139845, 0.07139841, 0.31990385, 0.05781847, 0.30887046,\n",
       "       0.21517548, 0.10266523, 0.1052977 , 0.17609064, 0.10253438,\n",
       "       0.0804897 , 0.1426054 , 0.1391601 , 0.05057511, 0.08691587,\n",
       "       0.06458239, 0.10722248, 0.0787853 , 0.30917954, 0.07386761,\n",
       "       0.1006656 , 0.10923968, 0.21782225, 0.07541741, 0.0901816 ,\n",
       "       0.08316138, 0.11798892, 0.14070423, 0.1586984 , 0.21846287,\n",
       "       0.15870616, 0.17795467, 0.08347883, 0.25167057, 0.16057757,\n",
       "       0.26159593, 0.11303103, 0.0910034 , 0.08102007, 0.06173062,\n",
       "       0.30791318, 0.14463773, 0.07282171, 0.21666066, 0.20883271,\n",
       "       0.08741899, 0.18231405, 0.1779998 , 0.1743781 , 0.17675932,\n",
       "       0.2107315 , 0.10983691, 0.18581906, 0.3011414 , 0.10917295,\n",
       "       0.13679817, 0.10683965, 0.24742593, 0.0987094 , 0.32997146,\n",
       "       0.0880011 , 0.17003399, 0.11081491, 0.06492715, 0.13743582,\n",
       "       0.26303503, 0.16964886, 0.06806266, 0.04839081, 0.06057626,\n",
       "       0.16030592, 0.13830422, 0.06557975, 0.24769959, 0.06989257,\n",
       "       0.10290067, 0.26697075, 0.03868598, 0.04487929, 0.25899047,\n",
       "       0.09940562, 0.15594676, 0.07447845, 0.07962967, 0.42510346,\n",
       "       0.06055994, 0.2773301 , 0.1239589 , 0.13578282, 0.0633575 ,\n",
       "       0.12335557, 0.25999194, 0.07034978, 0.09363037, 0.1853793 ,\n",
       "       0.21466908, 0.18558004, 0.1046758 , 0.08746391, 0.26401073,\n",
       "       0.1187819 , 0.22724749, 0.09320384, 0.17573732, 0.12585051,\n",
       "       0.1744392 , 0.14818577, 0.12364619, 0.07917378, 0.10602971,\n",
       "       0.13254835, 0.21370257, 0.1244022 , 0.1603048 , 0.08062808,\n",
       "       0.21573578, 0.05597295, 0.05618502, 0.09290682, 0.10737941,\n",
       "       0.21499857, 0.18090549, 0.1920294 , 0.18086618, 0.19533256,\n",
       "       0.06341324, 0.07570174, 0.09229071, 0.1683673 , 0.20403692,\n",
       "       0.08198416, 0.12584828, 0.19985002, 0.08384389, 0.09573581,\n",
       "       0.26552463, 0.36933804, 0.07374511, 0.13555424, 0.07836428,\n",
       "       0.20481865, 0.15599291, 0.0965993 , 0.26611394, 0.15513231,\n",
       "       0.15796895, 0.16421317, 0.06350153, 0.23474702, 0.13204505,\n",
       "       0.23391896, 0.23033448, 0.31141123, 0.3028761 , 0.20430928,\n",
       "       0.11133043, 0.08533077, 0.12861976, 0.29789075, 0.17467938,\n",
       "       0.18744133, 0.19725208, 0.08597869, 0.22757068, 0.05911585,\n",
       "       0.11902713, 0.05373533, 0.06888288, 0.0655344 , 0.11264123,\n",
       "       0.0833854 , 0.19327748, 0.04465984, 0.08867647, 0.09437679,\n",
       "       0.12746644, 0.10379701, 0.07929331, 0.07059804, 0.13362768,\n",
       "       0.07041991, 0.13835488, 0.07491338, 0.07240239, 0.18918069,\n",
       "       0.1811148 , 0.09359144, 0.30161572, 0.2319007 , 0.14579186,\n",
       "       0.05767756, 0.1881013 , 0.05793779, 0.08453178, 0.16773294,\n",
       "       0.38488474, 0.20278981, 0.19031015, 0.06196581, 0.15421452,\n",
       "       0.08927643, 0.19783999, 0.1442631 , 0.151855  , 0.13401125,\n",
       "       0.18593441, 0.04622347, 0.16831575, 0.3954651 , 0.09615326,\n",
       "       0.10875146, 0.21066657, 0.2539152 , 0.1198431 , 0.16390608,\n",
       "       0.22141504, 0.10361481, 0.17971322, 0.17447434, 0.31826937,\n",
       "       0.11047882, 0.05108324, 0.10701375, 0.14252023, 0.11272914,\n",
       "       0.21343353, 0.21521479, 0.17581138, 0.10898621, 0.14484206,\n",
       "       0.1274414 , 0.19308811, 0.18732509, 0.14283903, 0.10662249,\n",
       "       0.15863338, 0.15397964, 0.23915592, 0.11153334, 0.15693508,\n",
       "       0.14682917, 0.15091403, 0.1252916 , 0.24962421, 0.22019662,\n",
       "       0.08092507, 0.13870685, 0.11150564, 0.2918991 , 0.07915605,\n",
       "       0.1467446 , 0.31795546, 0.09787638, 0.08689715, 0.05141222,\n",
       "       0.06868022, 0.18138707, 0.1370166 , 0.13766465, 0.09661289,\n",
       "       0.06644977, 0.16401334, 0.10143099, 0.11141712, 0.09513331,\n",
       "       0.11170443, 0.09667498, 0.14499478, 0.13083902, 0.04970596,\n",
       "       0.09271666, 0.14071698, 0.2508475 , 0.08187698, 0.11095694,\n",
       "       0.20847121, 0.1340986 , 0.10519923, 0.15473829, 0.0919564 ,\n",
       "       0.09719492, 0.07400046, 0.14942665, 0.18823314, 0.27360573,\n",
       "       0.04532124, 0.23716623, 0.11832504, 0.18121283, 0.08585675,\n",
       "       0.12032946, 0.2048599 , 0.18026836, 0.04064543, 0.21634597,\n",
       "       0.10475626, 0.23601085, 0.249179  , 0.11675256, 0.1493324 ,\n",
       "       0.10284593, 0.08790556, 0.14192188, 0.05699385, 0.06036449,\n",
       "       0.10459163, 0.12893546, 0.09165043, 0.22135255, 0.0955395 ,\n",
       "       0.24614255, 0.12752023, 0.10404629, 0.11043695, 0.06606948,\n",
       "       0.07322074, 0.19616824, 0.17106573, 0.2290892 , 0.28240398,\n",
       "       0.16989109, 0.15754128, 0.14682834, 0.28551722, 0.06946576,\n",
       "       0.13568637, 0.20926291, 0.07840537, 0.05714308, 0.1613037 ,\n",
       "       0.08960382, 0.1339622 , 0.13050652, 0.08784208, 0.27151525,\n",
       "       0.27416244, 0.06587323, 0.26206353, 0.08137051, 0.20971191,\n",
       "       0.07906336, 0.18992203, 0.04324665, 0.11585239, 0.0830581 ,\n",
       "       0.11832965, 0.1977065 , 0.09258715, 0.37643465, 0.21464008,\n",
       "       0.08397368, 0.1655615 , 0.05240983, 0.23556828, 0.35774928,\n",
       "       0.13708512, 0.14969304, 0.07375306, 0.07432827, 0.14415535,\n",
       "       0.23037918, 0.07767535, 0.06308337, 0.14440994, 0.16278265,\n",
       "       0.13683617, 0.13005689, 0.1669539 , 0.09365302, 0.16437861,\n",
       "       0.209105  , 0.08479082, 0.20654725, 0.25599214, 0.12561287,\n",
       "       0.09200247, 0.10545965, 0.04633975, 0.13277218, 0.06145365,\n",
       "       0.14787601, 0.23083548, 0.14028038, 0.103487  , 0.06390011,\n",
       "       0.1508192 , 0.22303541, 0.10557894, 0.28724083, 0.1092144 ,\n",
       "       0.13752864, 0.16652793, 0.24969122, 0.33739465, 0.17301697,\n",
       "       0.13939838, 0.19378813, 0.13263774, 0.07991318, 0.10769155,\n",
       "       0.2940468 , 0.09595986, 0.09156776, 0.33715245, 0.1885965 ,\n",
       "       0.18496759, 0.1512866 , 0.1035305 , 0.084145  , 0.0555925 ,\n",
       "       0.11286485, 0.13738632, 0.16489604, 0.187428  , 0.20905504,\n",
       "       0.24868338, 0.17220536, 0.15153094, 0.13128188, 0.2353523 ,\n",
       "       0.13422325, 0.2663582 , 0.04217422, 0.19859582, 0.24658845,\n",
       "       0.09690781, 0.18550898, 0.1736057 , 0.29776564, 0.25162268,\n",
       "       0.40611544, 0.20205024, 0.14569992, 0.17076786, 0.18147981,\n",
       "       0.15567803, 0.05584165, 0.07760424, 0.10891742, 0.04375053,\n",
       "       0.31696904, 0.19946447, 0.18481775, 0.1451805 , 0.2584018 ,\n",
       "       0.20258874, 0.11869831, 0.13659574, 0.17193012, 0.06180535,\n",
       "       0.05667599, 0.30037823, 0.21336263, 0.07897991, 0.06851457,\n",
       "       0.05957834, 0.21355487, 0.32012773, 0.06510562, 0.2013894 ,\n",
       "       0.21656889, 0.10792842, 0.114819  , 0.21006766, 0.09713028,\n",
       "       0.17138477, 0.27172995, 0.15461536, 0.3030547 , 0.10605676,\n",
       "       0.28634736, 0.11301529, 0.09721907, 0.12765093, 0.21074384,\n",
       "       0.15246503, 0.10434391, 0.05173187, 0.06364381, 0.05403917,\n",
       "       0.0751284 , 0.11361361, 0.12216745, 0.1340844 , 0.09079851,\n",
       "       0.1600892 , 0.06566536, 0.1234521 , 0.180933  , 0.05503432,\n",
       "       0.22026557, 0.08572293, 0.09312977, 0.16308717, 0.09105745,\n",
       "       0.12019455, 0.16737626, 0.21238546, 0.08095255, 0.16243596,\n",
       "       0.2317505 , 0.04983025, 0.2706064 , 0.10032209, 0.12228361,\n",
       "       0.14477053, 0.25821048, 0.09335846, 0.12123667, 0.09815523,\n",
       "       0.11763416, 0.13821118, 0.11017218, 0.04048746, 0.06734776,\n",
       "       0.16003275, 0.25523046, 0.18297043, 0.2596288 , 0.08439531,\n",
       "       0.21561831, 0.09077899, 0.12608495, 0.20696585, 0.17665985,\n",
       "       0.07878572, 0.30618486, 0.0890564 , 0.05226821, 0.10570017,\n",
       "       0.1891403 , 0.10155378, 0.05439346, 0.34493023, 0.08845815,\n",
       "       0.14732613, 0.06658641, 0.21099988, 0.15859655, 0.23752497,\n",
       "       0.06214578, 0.31103355, 0.09652733, 0.2117662 , 0.17855597,\n",
       "       0.08844779, 0.07181847, 0.10102544, 0.29672986, 0.23903541,\n",
       "       0.17654479, 0.05666706, 0.05102313, 0.07929543, 0.05066892,\n",
       "       0.11582229, 0.09245434, 0.07815879, 0.14916667, 0.17754951,\n",
       "       0.06836188, 0.09672879, 0.05337345, 0.11410036, 0.25295153,\n",
       "       0.15986349, 0.13138929, 0.14098595, 0.28679636, 0.10224245,\n",
       "       0.16426876, 0.22038436, 0.1005887 , 0.11574642, 0.25223362,\n",
       "       0.14854814, 0.17589535, 0.13904312, 0.10445061, 0.05790222,\n",
       "       0.23454732, 0.06055496, 0.10403021, 0.2387226 , 0.13449663,\n",
       "       0.05575162, 0.13301048, 0.20372179, 0.19481437, 0.2648094 ,\n",
       "       0.09102038, 0.1094612 , 0.16206549, 0.06813669, 0.12019888,\n",
       "       0.3160553 , 0.08652096, 0.25828263, 0.29460448, 0.09238982,\n",
       "       0.07942921, 0.0696201 , 0.09412383, 0.12212668, 0.10466146,\n",
       "       0.2659606 , 0.15898347, 0.26590753, 0.2284513 , 0.10852826,\n",
       "       0.07428775, 0.21788917, 0.18593489, 0.2936523 , 0.17195626,\n",
       "       0.08227129, 0.25257105, 0.1115034 , 0.29536122, 0.04910312,\n",
       "       0.16194077, 0.3230038 , 0.13677397, 0.07587082, 0.09816367,\n",
       "       0.13026549, 0.16337912, 0.11905658, 0.27401915, 0.13507031,\n",
       "       0.22984284, 0.12249602, 0.19481003, 0.19701385, 0.07926314,\n",
       "       0.17439362, 0.09207443, 0.10248987, 0.19699132, 0.07232202,\n",
       "       0.10620436, 0.1959887 , 0.11415686, 0.09248188, 0.05195213,\n",
       "       0.10146133, 0.07969639, 0.14025486, 0.12031448, 0.08911348,\n",
       "       0.30819684, 0.09079923, 0.07996129, 0.13178506, 0.15579754,\n",
       "       0.13489991, 0.06482688, 0.08482676, 0.20270805, 0.0887202 ,\n",
       "       0.16424787, 0.11384169, 0.05893458, 0.07984435, 0.16205221,\n",
       "       0.0754825 , 0.11506846, 0.07161136, 0.17575492, 0.09920523,\n",
       "       0.14515296, 0.13117908, 0.3220194 , 0.1520403 , 0.23701589,\n",
       "       0.19971709, 0.13600998, 0.06618565, 0.10314503, 0.07456744,\n",
       "       0.1723361 , 0.14018305, 0.13616407, 0.3724521 , 0.2966387 ,\n",
       "       0.2812159 , 0.20086831, 0.08865106, 0.20866702, 0.12691037,\n",
       "       0.10585194, 0.1967787 , 0.17579229, 0.12700407, 0.12857243,\n",
       "       0.20299026, 0.10218476, 0.1649001 , 0.29569787, 0.38514316,\n",
       "       0.16053452, 0.11727869, 0.10516374, 0.08433894, 0.08247303,\n",
       "       0.0710062 , 0.19515558, 0.10402821, 0.07416543, 0.08627681,\n",
       "       0.08289739, 0.25338683, 0.23692119, 0.15676257, 0.11534982,\n",
       "       0.05265888, 0.04394629, 0.09100842, 0.1847542 , 0.08232528,\n",
       "       0.25786355, 0.10887804, 0.1605586 , 0.2737542 , 0.23276788,\n",
       "       0.22229384, 0.20012023], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extracting predictions\n",
    "mlp_pred1 = mlp1.predict(X_test)[:, 1]\n",
    "mlp_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24c1003a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12721303, 0.10498027, 0.14341049, 0.1920079 , 0.1808803 ,\n",
       "       0.18316041, 0.15272322, 0.12715995, 0.16787809, 0.14522839,\n",
       "       0.14900008, 0.118746  , 0.11212127, 0.11027972, 0.12110838,\n",
       "       0.12834103, 0.13301189, 0.18013509, 0.16565916, 0.11804674,\n",
       "       0.13313359, 0.12015658, 0.11715899, 0.22988373, 0.1177552 ,\n",
       "       0.14587101, 0.11810862, 0.19900353, 0.17720215, 0.16312732,\n",
       "       0.12091272, 0.11908252, 0.18500121, 0.14806874, 0.20143785,\n",
       "       0.17660637, 0.12946896, 0.09749887, 0.19069323, 0.23235115,\n",
       "       0.1407081 , 0.18388194, 0.18327603, 0.16411383, 0.19310555,\n",
       "       0.2141448 , 0.1939315 , 0.10316759, 0.12505913, 0.18758458,\n",
       "       0.11659352, 0.19729213, 0.1358218 , 0.12888326, 0.21693137,\n",
       "       0.14567919, 0.12413239, 0.14659682, 0.21578878, 0.16610515,\n",
       "       0.20519347, 0.14105153, 0.17752166, 0.17038634, 0.12146046,\n",
       "       0.13111863, 0.18518347, 0.1813964 , 0.19117273, 0.18489505,\n",
       "       0.23882322, 0.14627245, 0.12182318, 0.18261337, 0.18413217,\n",
       "       0.17072943, 0.19220993, 0.18300216, 0.15989165, 0.11446105,\n",
       "       0.12960713, 0.11486724, 0.13471347, 0.16323133, 0.18693623,\n",
       "       0.1471528 , 0.17146903, 0.11508714, 0.18194424, 0.1481304 ,\n",
       "       0.19783463, 0.13570748, 0.1192068 , 0.19493285, 0.1706205 ,\n",
       "       0.20197466, 0.1940613 , 0.17136571, 0.11839589, 0.12298187,\n",
       "       0.15213253, 0.10761288, 0.14635782, 0.18010658, 0.17804058,\n",
       "       0.15957984, 0.19363473, 0.14233361, 0.181309  , 0.14065371,\n",
       "       0.24061209, 0.14301296, 0.1773261 , 0.12908055, 0.18867931,\n",
       "       0.11246562, 0.13536847, 0.20684852, 0.18800376, 0.15889533,\n",
       "       0.1466493 , 0.17202449, 0.17785741, 0.16805966, 0.10615376,\n",
       "       0.13427585, 0.15491554, 0.14022791, 0.117507  , 0.16737154,\n",
       "       0.16386488, 0.26597112, 0.17867438, 0.21382342, 0.21368065,\n",
       "       0.14714852, 0.18046352, 0.217395  , 0.11205789, 0.13257444,\n",
       "       0.24061777, 0.16493101, 0.1915377 , 0.12506378, 0.11374731,\n",
       "       0.13452671, 0.17835684, 0.13838062, 0.22966252, 0.13011098,\n",
       "       0.18176548, 0.175536  , 0.18495889, 0.10810066, 0.20046943,\n",
       "       0.1654016 , 0.13019629, 0.24765088, 0.1903481 , 0.14261809,\n",
       "       0.1790806 , 0.13077635, 0.1727404 , 0.22114137, 0.10185183,\n",
       "       0.12382554, 0.15490729, 0.13008954, 0.20575446, 0.15436237,\n",
       "       0.18421035, 0.08455541, 0.18228185, 0.18544967, 0.14135213,\n",
       "       0.15852405, 0.17276998, 0.15899608, 0.20263675, 0.10430724,\n",
       "       0.18809724, 0.13593142, 0.18384458, 0.16947293, 0.13247041,\n",
       "       0.18016599, 0.1251009 , 0.12768681, 0.17834198, 0.10832711,\n",
       "       0.18803225, 0.11075866, 0.12591466, 0.11076931, 0.14217801,\n",
       "       0.14178514, 0.1343228 , 0.18061656, 0.11952755, 0.19752355,\n",
       "       0.15951115, 0.14370105, 0.13816892, 0.18592204, 0.1378966 ,\n",
       "       0.19967276, 0.17299059, 0.19266656, 0.18335438, 0.14498971,\n",
       "       0.22188711, 0.185127  , 0.20578526, 0.14642002, 0.17487521,\n",
       "       0.09020994, 0.23410235, 0.13242206, 0.12890606, 0.14525448,\n",
       "       0.19179009, 0.11300354, 0.21178305, 0.19602057, 0.15417717,\n",
       "       0.20544302, 0.14135352, 0.14501151, 0.12720115, 0.13309923,\n",
       "       0.13797985, 0.15511212, 0.20538902, 0.18960015, 0.23401463,\n",
       "       0.15535244, 0.1911652 , 0.18744154, 0.10977872, 0.11468346,\n",
       "       0.2225934 , 0.13791987, 0.20058545, 0.17114623, 0.19209056,\n",
       "       0.20353985, 0.19436166, 0.1991036 , 0.18904296, 0.15429848,\n",
       "       0.12659918, 0.11812286, 0.13783151, 0.14473596, 0.11006801,\n",
       "       0.22511275, 0.20054264, 0.12519711, 0.12394984, 0.15395615,\n",
       "       0.1205989 , 0.09626772, 0.22703895, 0.1869724 , 0.12594776,\n",
       "       0.18756962, 0.1433977 , 0.1895613 , 0.17426991, 0.17664433,\n",
       "       0.15178679, 0.1459641 , 0.13618352, 0.1897697 , 0.19123405,\n",
       "       0.18284974, 0.12308382, 0.1633649 , 0.1045849 , 0.20087354,\n",
       "       0.13929883, 0.13266045, 0.13511916, 0.18411425, 0.22768514,\n",
       "       0.14164469, 0.1567615 , 0.16702415, 0.2210435 , 0.1898898 ,\n",
       "       0.15941069, 0.22898266, 0.09550003, 0.12237352, 0.17595986,\n",
       "       0.1776816 , 0.11847531, 0.12981352, 0.17205717, 0.13882048,\n",
       "       0.1319158 , 0.12821624, 0.1049271 , 0.23553127, 0.15139309,\n",
       "       0.13701153, 0.13976721, 0.12663968, 0.1974771 , 0.12004793,\n",
       "       0.22459148, 0.11226548, 0.13381314, 0.23482613, 0.15521854,\n",
       "       0.16121846, 0.1778527 , 0.12898794, 0.19428305, 0.18435591,\n",
       "       0.21045962, 0.22662565, 0.23867798, 0.10088506, 0.21790013,\n",
       "       0.20656183, 0.13912399, 0.15704688, 0.21099642, 0.16420618,\n",
       "       0.13589989, 0.2230521 , 0.12814426, 0.18370983, 0.21161278,\n",
       "       0.13668784, 0.15359186, 0.21798494, 0.19890326, 0.12088249,\n",
       "       0.1247867 , 0.17579849, 0.20395274, 0.15966251, 0.18912023,\n",
       "       0.14738882, 0.14749268, 0.14643209, 0.1230325 , 0.17841366,\n",
       "       0.13125095, 0.19285536, 0.1258428 , 0.21170309, 0.23062842,\n",
       "       0.13021767, 0.19419187, 0.16521262, 0.18892366, 0.14004259,\n",
       "       0.11141323, 0.13645385, 0.203905  , 0.1967056 , 0.16606666,\n",
       "       0.19984433, 0.18849635, 0.17450948, 0.20934635, 0.18445282,\n",
       "       0.20029292, 0.18827677, 0.16648799, 0.11723474, 0.20124958,\n",
       "       0.13565283, 0.1303667 , 0.23792681, 0.19477563, 0.18825503,\n",
       "       0.21710624, 0.1994019 , 0.13225769, 0.19394152, 0.19503762,\n",
       "       0.17529519, 0.14548773, 0.13308448, 0.11933422, 0.17198154,\n",
       "       0.2126329 , 0.21609177, 0.18137957, 0.23358542, 0.13222225,\n",
       "       0.16663413, 0.14524561, 0.11243077, 0.21463875, 0.13682559,\n",
       "       0.19904119, 0.11749102, 0.14782853, 0.13555358, 0.12363503,\n",
       "       0.12873998, 0.14359297, 0.09423672, 0.16191216, 0.15243971,\n",
       "       0.12906724, 0.13999087, 0.18088837, 0.11988825, 0.15870954,\n",
       "       0.12402452, 0.15444386, 0.19237994, 0.15268843, 0.14854865,\n",
       "       0.1344634 , 0.19176687, 0.11555327, 0.19300158, 0.18452325,\n",
       "       0.16864656, 0.19877404, 0.17791687, 0.16163239, 0.22554882,\n",
       "       0.12978691, 0.13719898, 0.16533917, 0.17804615, 0.13517535,\n",
       "       0.13451222, 0.21175392, 0.18057986, 0.23004182, 0.19421221,\n",
       "       0.11698623, 0.18359663, 0.12926711, 0.16453631, 0.14522587,\n",
       "       0.12481663, 0.16471057, 0.13168098, 0.19187996, 0.18756828,\n",
       "       0.18535036, 0.20262527, 0.09801783, 0.16953626, 0.12532578,\n",
       "       0.12114008, 0.23361078, 0.20297302, 0.14145713, 0.16221562,\n",
       "       0.18172885, 0.21484236, 0.1532735 , 0.1653932 , 0.2066776 ,\n",
       "       0.17172728, 0.1132344 , 0.13946198, 0.20948438, 0.19056897,\n",
       "       0.1450401 , 0.2257198 , 0.13312595, 0.23312919, 0.12259327,\n",
       "       0.18355107, 0.11999896, 0.17356616, 0.14264604, 0.20910113,\n",
       "       0.17751507, 0.12067532, 0.17129146, 0.09931745, 0.1591181 ,\n",
       "       0.10883608, 0.1908488 , 0.12955101, 0.1944262 , 0.11882635,\n",
       "       0.12520729, 0.0861731 , 0.11108299, 0.121873  , 0.17273973,\n",
       "       0.13345024, 0.118746  , 0.16248897, 0.1990381 , 0.17485985,\n",
       "       0.18761526, 0.22577783, 0.17704645, 0.12945096, 0.09705497,\n",
       "       0.12782486, 0.17691106, 0.18986963, 0.16871268, 0.15546358,\n",
       "       0.1323947 , 0.17332602, 0.12551671, 0.15988944, 0.11273606,\n",
       "       0.12533781, 0.16403356, 0.19876277, 0.16049904, 0.19555846,\n",
       "       0.15427248, 0.20156756, 0.13360968, 0.19056112, 0.21362029,\n",
       "       0.14273153, 0.14646703, 0.19349667, 0.20800962, 0.12166104,\n",
       "       0.11545248, 0.20447898, 0.17977469, 0.1523848 , 0.1272169 ,\n",
       "       0.14322887, 0.12582387, 0.15019564, 0.2046548 , 0.128271  ,\n",
       "       0.13068506, 0.16153207, 0.20511694, 0.18004815, 0.13607708,\n",
       "       0.17522949, 0.22241805, 0.11343142, 0.10629394, 0.2192239 ,\n",
       "       0.15204284, 0.1291582 , 0.13646829, 0.13906725, 0.11857283,\n",
       "       0.14971209, 0.15172234, 0.18204397, 0.13048384, 0.15602654,\n",
       "       0.1004646 , 0.12044396, 0.1809505 , 0.16248862, 0.24301095,\n",
       "       0.13907312, 0.12089001, 0.16029084, 0.19880126, 0.12884243,\n",
       "       0.13063124, 0.123027  , 0.15422238, 0.16779222, 0.12741035,\n",
       "       0.2127268 , 0.20387074, 0.136268  , 0.1199107 , 0.22338222,\n",
       "       0.14609532, 0.11867803, 0.14551364, 0.14035006, 0.18173692,\n",
       "       0.12976794, 0.09353986, 0.19607748, 0.1927666 , 0.1599775 ,\n",
       "       0.19014758, 0.18312027, 0.20064214, 0.15228225, 0.1964144 ,\n",
       "       0.13019921, 0.19033675, 0.13922441, 0.12677489, 0.11324441,\n",
       "       0.22989103, 0.10217053, 0.17254783, 0.14634976, 0.13495515,\n",
       "       0.18688464, 0.08859783, 0.13606624, 0.2346048 , 0.10480332,\n",
       "       0.1372575 , 0.18045965, 0.22287068, 0.17292151, 0.11421104,\n",
       "       0.18306787, 0.21760477, 0.2508561 , 0.14409553, 0.13649884,\n",
       "       0.14585084, 0.14325826, 0.211706  , 0.14404532, 0.18739642,\n",
       "       0.12530956, 0.14747822, 0.1823373 , 0.18332055, 0.13872032,\n",
       "       0.12011893, 0.22926086, 0.13823007, 0.17728406, 0.12174612,\n",
       "       0.20592922, 0.14087668, 0.12387627, 0.15713659, 0.11640918,\n",
       "       0.20028548, 0.15747043, 0.13510443, 0.12780069, 0.11401502,\n",
       "       0.1338795 , 0.22111729, 0.17531471, 0.12379978, 0.17249852,\n",
       "       0.12384924, 0.1516508 , 0.16310619, 0.22883928, 0.18957548,\n",
       "       0.21812153, 0.18340376, 0.12242984, 0.18407246, 0.19172315,\n",
       "       0.10952102, 0.12235533, 0.12464955, 0.20219478, 0.19873847,\n",
       "       0.12726581, 0.16571839, 0.17009056, 0.11719985, 0.18658309,\n",
       "       0.17885824, 0.1285776 , 0.12044089, 0.17518882, 0.1322717 ,\n",
       "       0.13667075, 0.16885759, 0.15469906, 0.21118331, 0.13681597,\n",
       "       0.23940423, 0.12561134, 0.0834778 , 0.17012598, 0.20401284,\n",
       "       0.12312246, 0.12023427, 0.13740255, 0.15990497, 0.22708933,\n",
       "       0.15852432, 0.12552117, 0.12614104, 0.11343804, 0.10483227,\n",
       "       0.13078433, 0.18896176, 0.17916855, 0.11459775, 0.19692151,\n",
       "       0.16647267, 0.14238895, 0.17294346, 0.13432308, 0.13984331,\n",
       "       0.15129422, 0.14854829, 0.13678056, 0.20417458, 0.18472944,\n",
       "       0.1820922 , 0.1306665 , 0.12686406, 0.16387479, 0.11294943,\n",
       "       0.19009629, 0.15163946, 0.184474  , 0.16131236, 0.22600634,\n",
       "       0.13987966, 0.18294154, 0.15144892, 0.13710567, 0.17699133,\n",
       "       0.13055383, 0.19284028, 0.20293176, 0.17274894, 0.15923502,\n",
       "       0.2442189 , 0.20616762], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extracting predictions\n",
    "mlp_pred2 = mlp2.predict(X_test)[:, 1]\n",
    "mlp_pred2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
